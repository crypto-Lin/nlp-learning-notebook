### Loss Function Recap

[常用5个损失函数](https://www.jiqizhixin.com/articles/2018-06-21-3)

机器学习领域，深度学习领域，无论推荐算法模型，图像处理，还是自然语言理解，损失函数的定义这个环节都是非常核心的部分，一个好的损失函数不仅能够直接提升模型的表现，也能在训练过程中帮助模型更快地收敛。

本质上来讲，损失函数反映的是模型预测结果与真实结果之间的差距，这个差距当然是越小越好。所以数学上“距离”的概念都可以使用到损失函数的定义里。

##### Mean Square Error
比如回归预测问题当中常使用平方和（均方误差）来定义损失函数，不使用平方和开根，即直接使用欧式距离，完全是因为不方便之后模型的训练。MSE也叫L2损失。
$$MSE = \sum_{i=1}^n (y_i-\hat y_i)^2$$
$\hat y_i$ 代表模型预测值。

##### Mean Average Error
平均绝对值误差 (MAE) 也叫L1损失，即使用曼哈顿距离来定义损失函数，衡量了误差的平均模长，而没有考虑方向。如果考虑方向，则是残差／误差的总和，叫平均偏差（MBE）：

$$MAE = \sum_{i=1}^n |y_i - \hat y_i|$$
$$MBE = \sum_{i=1}^n (y_i - \hat y_i)$$

以上是对于回归问题常常使用的损失函数形式。由于MSE使用的平方和，对于异常点MSE会比MAE更加敏感的捕捉到，这会使MSE模型赋予异常点更大权重，更关注这些异常。所以如果数据是被污染的，MSE反而不如MAE有更好的鲁棒性。不过MAE并不能使用在神经网络里，因为线性函数的梯度是常数，即使对于很小的损失，梯度也很大，不利于模型的学习。因此在实际运用过程中，如果要挑选出异常，则选择MSE，但是如果要过滤掉异常，应选择MAE。

以下则是对于分类问题里常常使用的损失函数：

##### Cross Entropy

在二分类问题中，损失函数定义为最大似然函数的对数形式并取反：
$$f_{loss} = - \sum_i y_ilog(\hat y_i) + (1-y_i)log(1-\hat y_i)$$
这其实也是交叉熵的形式：
$$H_p(q) = -\sum_{i}^n p(x_i)logq(x_i)$$
其中$p(x)$代表真实分布，$q(x)$代表模型预测的分布，交叉熵能够反映二者分布的相似度。以交叉熵为形式的损失函数不只用于二分类问题，也用于softmax的多分类问题中 (categorical crossentropy)：
$$f_{loss} = -\sum_i y_ilog\hat y_i$$
需要注意的是这里i代表的是多分类的类别总数，不是样本／batch总数。

##### KL Divergence

KL散度又称为相对熵，定义如下：

$$D_{KL}(p||q) = Ep[log\frac{p(x)}{q(x)}] = \sum _x p(x)log\frac{p(x)}{q(x)} 
=\sum_x p(x)logp(x) - p(x)logq(x) $$
$$D_{KL}(p||q) = -H(p)+H_p(q)$$
KL散度表达的是真实分布p的前提下，使q编码相较p分布（最优分布）多出的bit数。

##### Exponential Loss
指数分布形式的损失函数常用在Adaboost算法中，标准形式如下：
$$L(Y|f(X)) = exp[-yf(x)]$$





