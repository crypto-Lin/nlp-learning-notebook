### Loss Function Recap

机器学习领域，深度学习领域，无论推荐算法模型，图像处理，还是自然语言理解，损失函数的定义这个环节都是非常核心的部分，一个好的损失函数不仅能够直接提升模型的表现，也能在训练过程中帮助模型更快地收敛。

本质上来讲，损失函数反映的是模型预测结果与真实结果之间的差距，这个差距当然是越小越好。所以数学上“距离”的概念都可以使用到损失函数的定义里。

##### Mean Square Error
比如回归预测问题当中常使用平方和（均方误差）来定义损失函数，不使用平方和开根，即直接使用欧式距离，完全是因为不方便之后模型的训练。MSE也叫L2损失。
$$MSE = \sum_{i=1}^n (y_i-\hat y_i)^2$$
$\hat y_i$ 代表模型预测值。

##### Mean Average Error
平均绝对值误差 (MAE) 也叫L1损失，即使用曼哈顿距离来定义损失函数，衡量了误差的平均模长，而没有考虑方向。如果考虑方向，则是残差／误差的总和，叫平均偏差（MBE）：

$$MAE = \sum_{i=1}^n |y_i - \hat y_i|$$
$$MBE = \sum_{i=1}^n (y_i - \hat y_i)$$

以上是对于回归问题常常使用的损失函数形式。以下则是对于分类问题里常常使用的损失函数：

##### Cross Entropy

在二分类问题中，损失函数定义为最大似然函数的对数形式并取反：
$$f_{loss} = - \sum_i y_ilog(\hat y_i) + (1-y_i)log(1-\hat y_i)$$
这其实也是交叉熵的形式：
$$H_p(q) = -\sum_{i}^n p(x_i)logq(x_i)$$
其中$p(x)$代表真实分布，$q(x)$代表模型预测的分布，交叉熵能够反映二者分布的相似度。以交叉熵为形式的损失函数不只用于二分类问题，也广泛用于多分类问题中。

